---
title: "Challenge Feux de Forêts"
author: "David Makowski & Eric Parent"
date: "The 8th of March 2021"
fontsize: 9pt
output: 
  beamer_presentation:
    dev: cairo_pdf
    highlight: tango
    includes:
      in_header: utils_header_presentation.tex
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      fig.align = "center",
                      cache = TRUE,
                      comment = NA,
                      out.width = "90%",
                      R.options = list(width = 120))
```

# Context

- Wildfires Challenge organizd by Thomas Opitz, Biosp, Avignon;
- Large dataset for Extreme and Spatio-Temporal modelling over continental United States
- Data is available at  https://www.maths.ed.ac.uk/school-of-mathematics/eva-2021/competitions/data-challenge, after registration to the competition.

```{r code_source_donnees_exploration, include = FALSE}

rm(list=ls())
source("code_chargement_donnees.R")

```

# Structure of the design 

- Spatio-temporal Lattice;
- $3503$ *sites* ( a $0.5 \times 0.5$ grid of longitude and latitude coordinates) are followed during $7$ months (March-September) during $23$ years (1993-2015),

- Overall $563983$ records;

```{r plan_exp, echo = FALSE, message = FALSE}
library(USAboundaries)
library(sf)
C18<-us_boundaries(as.Date("2000-01-01"))
C18d <- fortify(C18)
  
#  data_train_DF %>% 
#  filter(year==1999,month==8) %>% 
#  ggplot(aes(x=lon, y=lat, color=CNT))+
# geom_point()+
#    theme(legend.position="none") 
  
  ggplot(data=C18d %>% filter(name != "Alaska") %>% filter(name != "Hawaii"))+geom_sf(color="red")+geom_tile(data=data_train_DF %>% 
  filter(year==1999,month==8), aes(x=lon, y=lat, color=CNT), alpha=0.5)+
    theme(legend.position="none") 

```
# Targeted quantity : Counts

- On each site, at each time-step, the number of wildfires are recorded;

```{r print_number, message=FALSE, warning=FALSE}
data_train_DF %>% 
  filter(year==2009,month==8) %>% 
  ggplot(aes(x=lon, y=lat))+
  geom_tile(aes(fill=-log(CNT+1)), color="blue")+
  scale_fill_distiller(palette="Reds")+ theme(legend.position="none") +labs(title="August 2009 : Number of Wildfires (Logscale)", x="Longitude", y="Latitude")
```


# Targeted quantity : Damages 

- On each site, at each time-step, the number of aggregated burnt area of wildfires are recorded;

```{r print_surface, message=FALSE, warning=FALSE}
data_train_DF %>% 
  filter(year==2009,month==7) %>% 
  ggplot(aes(x=lon, y=lat))+
  geom_tile(aes(fill=-log(1+BA)), color="red")+
  scale_fill_distiller(palette="Blues")+ theme(legend.position="none") +labs(title="July 2009 : Aggregated burnt area by wildfires (Logscale)", x="Longitude", y="Latitude")
```

# Burnt area and wildfire counts are correlated

- with due respect to their actual nature

```{r print_maxmean, message=FALSE, warning=FALSE}
data_train_DF %>% group_by(site) %>% 
  summarize(lat=mean(lat),lon=mean(lon),CNTmax=max(CNT, na.rm=T), CNTmean=mean(CNT, na.rm=T),
            BAmax=max(BA, na.rm=T), BAmean=mean(BA, na.rm=T)) %>%
  arrange(desc(CNTmean) ) %>% relocate(lat,lon, .after= BAmean) %>% head()
```


# Six time-series profiles picked at random

```{r print_time_CNT1, message=FALSE, warning=FALSE}
d <- data_train_DF %>% group_by(site) %>% 
  summarize(lat=mean(lat),lon=mean(lon),CNTmax=max(CNT, na.rm=T), CNTmean=mean(CNT, na.rm=T),
            BAmax=max(BA, na.rm=T), BAmean=mean(BA, na.rm=T)) 

d %>% pull(CNTmean) ->CNTmm

data_train_DF<-data_train_DF %>%left_join(d)

set.seed(4)
d<- data_train_DF  %>%
  relocate(date) %>% filter(CNTmean > 4*CNTmm) %>% group_by(site) %>% nest() %>% ungroup() %>% 
  sample_n(6) %>% unnest(cols = c(data)) 

p1<-data_train_DF %>% 
  filter(year==2009,month==8) %>% 
  ggplot(aes(x=lon, y=lat))+
  geom_tile( fill="white",color="black")+
  geom_tile( data=d,aes(x=lon, y=lat, fill=site),color="black")+ theme(legend.position="none")#+ coord_fixed()
p2<-d %>% ggplot(aes(x=date, color=site,y=log(1+CNT)))+geom_line()+
 geom_point(aes(size=log(1+BA)))+facet_wrap(.~site, scales="free")+ theme(legend.position="none") +labs(title="Six  randomly drawn sites with Wildfires Counts (y axis) and Burnt Area (point size)", y="Wildfires Counts (log scale)")
#egg::ggarrange(p1, p2, widths = c(1,3))
p1
```

# Various time-series profiles

```{r print_time_CNT2, message=FALSE, warning=FALSE}
d %>% ggplot(aes(x=date, color=site,y=log(1+CNT)))+geom_line()+
  geom_point(aes(size=log(1+BA)))+facet_wrap(.~site)+ theme(legend.position="none") +labs(title="Six randomly drawn sites with Wildfires Counts (y axis) and Burnt Area (point size)", y="Wildfires Counts (log scale)")
```

# Explanatory variables

- 18 land cover variables containing proportions of land usages
 such as cropland rainfed, cropland rainfed herbaceous cover, ..., tree needleleave evergreen , ..., 
    grassland,  urban, bare areas,  water.
    
    
- 10 ERA5-reanalysis climate variables such as 10m components of wind, Dewpoint,  Temperature, ...Evaporation, Precipitation. 
    
```{r}
data_train_DF %>% select(-site,-(lc4:lc18),-(clim4:clim10), -CNTmax,-CNTmean,-BAmax,-BAmean)

```
    
    
# The challenge : predict $2\times 80000$ missing observations

BA and CNT data is missing for uneven years (1993, 1995…,2015). 

For even years (1994, 1996, …, 2014), overall $80,000$ observations of each of the two variables are masked for validation (NA in the dataset)

```{r echo=TRUE}
dim(data_train_DF)
sum(is.na(data_train_DF$BA))
sum(is.na(data_train_DF$CNT))
sum(is.na(data_train_DF$BA)*is.na(data_train_DF$CNT))
```

# (Weighted) Probabilistic Ranked Scores $WPRS_{BA}+WPRS_{CNT}$

The variable $Y$ to predict (Burnt Area BA and Counts CNT is binned into intervals 

$({u_k})_{k=1:K}$. 

Let's denote $\hat{p}_k$ the estimate of $\mathbb{P}(Y<u_k)$.

The score for $n$ sites where $y$ is to be predicted will be computed as:
$$WPRS_Y=\sum_{i=1}^n\sum_{k=1}^k \omega_k({1}_{y \leq u_k}-\hat{p}_k)^2$$ 
with weights  $\omega_k$ taylored for Extreme Value Analysis
$$\omega_k \propto 1-(1+(1+u_k)/1000)^{-\frac{1}{4}}$$
```{r echo=TRUE}
# show severity thresholds :
u_ba # for BA
# show weights used for the weighted RPS:
weights_ba
```

# Objectives

- Thomas would like to pinpoint the interest of Spatio-Temporal modelling of Extreme values.
- We (Eric et David) take this as a really nice data set to work out our modelling skills!
- David would favor modern robust techniques such as *Random Forest* techniques.
- Eric will focus on old *Bayesian learning with normal copulas*.